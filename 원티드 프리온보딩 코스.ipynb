{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "수강생 선발과제 가이드라인_AIB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mb07iIHm4JNz"
      },
      "outputs": [],
      "source": [
        "### 최대한 외부 라이브러리를 사용하지 않고, 코드를 작성했습니다.\n",
        "\n",
        "import re\n",
        "from math import log\n",
        "import pandas as pd\n",
        "\n",
        "class Tokenizer():\n",
        "    def __init__(self):\n",
        "        self.word_dict = {'oov': 0}\n",
        "        self.fit_checker = False\n",
        "\n",
        "    def preprocessing(self, sequences):\n",
        "\n",
        "        result = []  # 전처리한 데이터를 담을 리스트 선언.\n",
        "        for sequence in sequences :   # 입력된 각 문장에 아래와 같은 전처리 실행.\n",
        "            sequence = sequence.lower()  # 소문자화.\n",
        "            sequence = re.sub('[^a-zA-Zㄱ-ㅎ가-힣]', ' ', sequence)  # 특수문자 제거\n",
        "            sequence = sequence.split()   # 문장을 공백 기준으로 리스트로 변환.\n",
        "            result.append(sequence)   # result 리스트에 추가.\n",
        "        return result\n",
        "  \n",
        "    def fit(self, sequences):\n",
        "        self.fit_checker = False\n",
        "\n",
        "        sentences = self.preprocessing(sequences)  # 클래스 내부 함수로 전처리함.\n",
        "        num = 1   # vocab 구성에서 self.word_dict을 사용. 숫자0은 oov로 할당되어 숫자 1부터 key값 할당.\n",
        "        for sentence in sentences :\n",
        "            for word in sentence:\n",
        "                if word not in self.word_dict :  # self.word_dict의 딕셔너리에 해당 어휘가 없을 시, 추가.\n",
        "                    self.word_dict[word] = num\n",
        "                    num +=1\n",
        "\n",
        "        self.fit_checker = True\n",
        "  \n",
        "\n",
        "    def transform(self, sequences):\n",
        "        result = []\n",
        "        preprocessed_sentences = self.preprocessing(sequences)   # 클래스 내부 함수 호출하여 전처리함.\n",
        "\n",
        "        if self.fit_checker:   # 클래스 내부 fit함수로 vocab이 미리 구성될 경우.\n",
        "            for sentence in preprocessed_sentences :\n",
        "                sub_result = []  \n",
        "                for word in sentence :\n",
        "                    if word in self.word_dict :   \n",
        "                        sub_result.append(self.word_dict[word])   # 입력 문장이 어휘집합에 있을 경우 해당 토큰을 부여.\n",
        "                    else:\n",
        "                        sub_result.append(self.word_dict['oov'])   # 입력 문장이 어휘집합에 없을 경우 \"OOV\"를 부여.\n",
        "                result.append(sub_result)\n",
        "            return result\n",
        "        else:\n",
        "            raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        result = self.transform(sequences)\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "class TfidfVectorizer:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.N = 0   # 입력 문장의 수\n",
        "        self.vocab = []   # 입력 시퀀스로 토큰 집합 구성\n",
        "        self.fit_checker = False\n",
        "\n",
        "        self.idf_list = []  # idf 값을 저장할 리스트\n",
        "        self.tf_list = []   # tf 값을 저장할 리스트\n",
        "        self.tfidf_matrix = []  # tf-idf 값 저장할 리스트\n",
        "        \n",
        "    def make_vocab(self, tokenized) :  \n",
        "        # Tokenizer 클래스로 전처리된 시퀀스로 토큰 집합 구성\n",
        "\n",
        "        self.N = len(tokenized)  # 입력 시퀀스의 수 선언\n",
        "        vocab_list = []   \n",
        "        for i in range(self.N) :  # 입력 시퀀스를 하나의 리스트로 받음.\n",
        "            vocab_list += tokenized[i]\n",
        "            \n",
        "        vocab = list(set(vocab_list))  # 중복값 제거\n",
        "        vocab.sort()    # 토큰 집합 정렬\n",
        "        self.vocab = vocab   # 클래스 내에 토큰 집합 저장\n",
        "\n",
        "\n",
        "    def fit(self, sequences):\n",
        "        tokenized = self.tokenizer.fit_transform(sequences)\n",
        "        # Tokenizer 클래스를 사용하여 입력 데이터를 전처리함.\n",
        "\n",
        "        self.make_vocab(tokenized)  # 토큰 집합 구성\n",
        "        \n",
        "\n",
        "        result = []  # idf 값을 저장할 리스트 선언.\n",
        "\n",
        "        for j in range(len(self.vocab)) :\n",
        "            token = self.vocab[j]   # 토큰 집합을 하나씩 호출\n",
        "            df = 0\n",
        "            for k in tokenized :\n",
        "                df += token in k    # 해당 토큰이 등장한 문장(시퀀스)의 수\n",
        "            result.append(log(self.N/(df+1)))  # idf 값 구함.\n",
        "\n",
        "        self.idf_list = result    # idf 값 저장.\n",
        "        self.fit_checker = True\n",
        "\n",
        "\n",
        "    def transform(self, sequences):\n",
        "\n",
        "        if self.fit_checker:\n",
        "            tokenized = self.tokenizer.transform(sequences)\n",
        "            \n",
        "            tf_result = []   # tf 값을 저장할 리스트.\n",
        "            tfidf_result = []  # tfidf 값을 저장할 리스트.\n",
        "\n",
        "            for i in range(self.N) :\n",
        "                tf_result.append([])  # nested list 형태로 출력하기 위해 리스트 선언.\n",
        "                tfidf_result.append([])  # nested list 형태로 출력하기 위해 리스트 선언.\n",
        "\n",
        "                doc = tokenized[i]  # 시퀀스를 순서대로 호출.\n",
        "                for j in range(len(self.vocab)) :  # 토큰 집합을 순서대로 호출.\n",
        "                    token = self.vocab[j]\n",
        "\n",
        "                    tf = doc.count(token)   # 각 시퀀스에 해당 토큰의 수를 셈.\n",
        "                    tf_result[-1].append(tf)  # \n",
        "\n",
        "                    idf = self.idf_list[token - 1]  \n",
        "                    # 토큰 '1'에 대한 idf_list의 값은 0번째이므로 1씩 빼줘서 인덱싱함.\n",
        "                    tfidf_result[-1].append(tf*idf)\n",
        "                    \n",
        "\n",
        "            self.tf_list = tf_result   # tf 값 저장\n",
        "            self.tfidf_matrix = tfidf_result   # TF-IDF 값 저장.\n",
        "            \n",
        "            return self.tfidf_matrix\n",
        " \n",
        "        else:\n",
        "            raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "    def fit_transform(self, sequences):\n",
        "        self.fit(sequences)\n",
        "        return self.transform(sequences)   \n",
        "\n",
        "\n",
        "\n",
        "### testing\n",
        "\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_transform(['I go to school.', 'I LIKE pizza!'])\n",
        "# tokenizer.transform(['I go to home.', 'do you like chicken?',])\n",
        "\n",
        "# tokenizer = Tokenizer()\n",
        "# tfidf = TfidfVectorizer(tokenizer)\n",
        "# test = ['I go to home.', 'do you like chicken?', 'i go to korea', 'chicken chicken i like', ' korea korea i you']\n",
        "# tfidf.fit_transform(test)"
      ]
    }
  ]
}